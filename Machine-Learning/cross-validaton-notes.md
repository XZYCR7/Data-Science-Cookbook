


> Written with [StackEdit](https://stackedit.io/).

> General advice for big dataset usually k = 3 or k = 5 is a preferred option while in small datasets it is recommended to use Leave one out.

Reference: [Cross validation](https://towardsdatascience.com/cross-validation-70289113a072)

> Larger K means less bias towards overestimating the true expected error (as training folds will be closer to the total dataset) but higher variance and higher running time (as you are getting closer to the limit case: Leave-One-Out CV).

Reference: [how to calculate the fold number k fold in cross validation](https://datascience.stackexchange.com/questions/28158/how-to-calculate-the-fold-number-k-fold-in-cross-validation)


<!--stackedit_data:
eyJoaXN0b3J5IjpbLTczMjk4NTk1Ml19
-->