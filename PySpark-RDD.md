> Written with [StackEdit](https://stackedit.io/).
## RRDs using Spark 2.0+

- RDDs are schema-less structures that can help you handle both structured and unstructured data.
- RDDs are schema-less distributed collection of immutable JVM objects. They were available from the very beginning, since Matei Zaharia started working on Spark. 
- Simplifying a little bit, similarly to how Hadoop works, RDDs chunk the data into partitions and each executor receives a portion of the data. 

## Creation RDDs
In simple terms, there are two ways to create an RDD, you can either parallelize a collection such as a list of values of tuples or you can read the data from a file, we'll cover both ways.

### Parallelize a Collection
The easiest way to create an RDD is to parallelize a collection. In this first example, we first load a NumPy library and then create 1000 random objects, random numbers.

```python
%pyspark
import numpy as np

random_numbers = sc.parallelize([np.random.rand() for _ in range(1000)],4)
random_numbers]
```
Output:
```
ParallelCollectionRDD[5] at parallelize at PythonRDD.scala:489
```
```python
%pyspark
random_numbers.take(2)
```
Output:
```
[0.0013382116436170266, 0.8232194124857617]
```
`sc` is the Spark context object, it is a first entry point to access the Spark's functionality. It also allows us to create RDDs as you can see. Spark context gets created automatically if you run PySpark in Jupyter/Zeppelin. 

The `parallelize` method takes at least one parameter and it is the collection that you want to parallelize. In our example here, it is a list of 1000 random numbers generated by the NumPy random.rand method. If you don't know Python, what you're looking here at is a construct known as a list comprehension, the range method creates a list of 1000 consecutive numbers starting from 0 that we look through. However, we do not use them in anyway so we only want the random numbers so that's why we don't even name the loop variable, that's why we use underscore. The second parameter we pass to the parallelize method, the number 4 over there, specifies how many slices to chunk the data into. This is an optional parameter so if you run that without this parameter, it will still create an RDD. Well, if you now hit Control + Enter, the code will run and you will see that we have just created the ParallelCollectionRDD, just like here.
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTg0NDg3MzcxNSwtODExNDc4MTcsMTExMD
AwMzE3Ml19
-->